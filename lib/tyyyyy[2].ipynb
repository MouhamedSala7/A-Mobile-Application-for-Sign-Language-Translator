{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "05/20/2024 02:29:36 - INFO - happytransformer.happy_transformer -   Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText\n",
    "happy_tt = HappyTextToText(\"T5\", \"prithivida/grammar_error_correcter_v1\")\n",
    "from happytransformer import TTSettings\n",
    "top_k_sampling_settings = TTSettings(do_sample=True, top_k=50, temperature=0.7, min_length=1, max_length=10)\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time  #Import the time module\n",
    "import re\n",
    "import gradio as gr\n",
    "import base64\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MediaPipe models for holistic and face detection\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Initialize the models\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.3)\n",
    "\n",
    "# Labels for hand gesture recognition\n",
    "labels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'Iam', 4: 'del', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L',\n",
    "                13: 'M', 14: 'N', 15: 'nothing', 16: 'O', 17: 'P', 18: 'Q', 19: 'R', 20: 'S', 21: ' ', 22: 'T', 23: 'U',\n",
    "                24: 'V', 25: 'W', 26: 'X', 27: 'Y', 28: 'Z', 29: '0', 30: '1', 31: '2', 32: '3', 33: '4', 34: '5', 35: '6',\n",
    "                36: '7', 37: '8', 38: '9', 39: 'Love' , 40: 'You' , 41: 'Ok' , 42: 'Thanks' , 43: 'How' , 44: 'are'}\n",
    "\n",
    "\n",
    "# Load your Keras model\n",
    "# Load the TFLite model\n",
    "# Load your Keras model\n",
    "model_dict = pickle.load(open('C:\\\\Users\\\\acer\\\\Downloads\\\\pj\\\\pj\\\\model.p', 'rb'))\n",
    "model = model_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sent(str):\n",
    "     words=re.findall(r'\\b\\w+\\b',str.lower())\n",
    "     new =words[0];\n",
    "     x=words[0]\n",
    "     new+=\" \"\n",
    "     for i in words:\n",
    "          if x!= i:\n",
    "             x=i\n",
    "             new+=i+\" \"\n",
    "     str=new+'.'\n",
    "    #  return str\n",
    "     result = happy_tt.generate_text(str, args=top_k_sampling_settings).text\n",
    "     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "def process_img(config):\n",
    " word = \"\"       \n",
    " data_aux = []\n",
    " x_ = []\n",
    " y_ = []\n",
    " # if it image path cv2.imread(config)\n",
    " frame = config\n",
    "\n",
    " H, W, _ = frame.shape\n",
    " frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    " # Perform face detection\n",
    " face_results = face_detection.process(frame_rgb)\n",
    "\n",
    " if face_results.detections:\n",
    "      for detection in face_results.detections:\n",
    "          bboxC = detection.location_data.relative_bounding_box\n",
    "          ih, iw, _ = frame.shape\n",
    "          bbox = (\n",
    "              int(bboxC.xmin * iw),\n",
    "              int(bboxC.ymin * ih),\n",
    "              int(bboxC.width * iw),\n",
    "              int(bboxC.height * ih),\n",
    "          )\n",
    "\n",
    "          cv2.rectangle(frame, bbox, (0, 255, 0), 2)\n",
    "\n",
    " # Perform hand and holistic detections\n",
    " results = holistic.process(frame_rgb)\n",
    " hand_results = hands.process(frame_rgb)\n",
    "    \n",
    " # Process hand landmarks\n",
    " if hand_results.multi_hand_landmarks:\n",
    "     for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "         mp_drawing.draw_landmarks(\n",
    "             frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "              mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "              mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "         for i in range(len(hand_landmarks.landmark)):\n",
    "             x = hand_landmarks.landmark[i].x\n",
    "             y = hand_landmarks.landmark[i].y\n",
    "             data_aux.append(x)\n",
    "             data_aux.append(y)\n",
    "             x_.append(x)\n",
    "             y_.append(y)\n",
    "\n",
    "     for i in range(len(hand_landmarks.landmark)):\n",
    "         x = hand_landmarks.landmark[i].x\n",
    "         y = hand_landmarks.landmark[i].y\n",
    "         data_aux.append(x - min(x_))\n",
    "         data_aux.append(y - min(y_))\n",
    "          \n",
    "\n",
    "     x1 = int(min(x_) * W) - 10\n",
    "     y1 = int(min(y_) * H) - 10\n",
    "     x2 = int(max(x_) * W) - 10\n",
    "     y2 = int(max(y_) * H) - 10\n",
    "\n",
    "     # Ensure that data_aux has the correct number of features\n",
    "     if len(data_aux) != 84:\n",
    "         print(\"Error: Incorrect number of features. Check your feature extraction process.\")\n",
    "     else:\n",
    "     # Make predictions using the model\n",
    "         prediction = model.predict([np.asarray(data_aux)])\n",
    "         predicted_character = labels_dict[int(prediction[0])]\n",
    "    \n",
    "            \n",
    "         if(len(predicted_character)==1):\n",
    "          return predicted_character\n",
    "                  \n",
    "         else :   \n",
    "          return (\" \" + predicted_character+\" \")\n",
    "\n",
    "        \n",
    "\n",
    " return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_msg(video_path, interval=2):\n",
    "    # Open the video file\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    output_folder = 'output_frames'\n",
    "\n",
    "    # Get the frame rate of the video\n",
    "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Calculate the frame interval in terms of frames\n",
    "    frame_interval = fps * interval\n",
    "    \n",
    "    # Initialize variables\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    frame_number = 0\n",
    "    word=\"\"\n",
    "    # Loop through the video frames\n",
    "    while success:\n",
    "        # Only save frames at specified intervals\n",
    "        if count % frame_interval == 0:\n",
    "            # Save the frame\n",
    "            cv2.imwrite(f\"{output_folder}/frame_{frame_number}.jpg\", image)\n",
    "            word+=process_img(image)\n",
    "            frame_number += 1\n",
    "        \n",
    "        # Read the next frame\n",
    "        success, image = vidcap.read()\n",
    "        count += 1\n",
    "    \n",
    "    # Release the video capture object\n",
    "    vidcap.release()\n",
    "    return correct_sent(word)\n",
    "    #return word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/20/2024 02:30:12 - INFO - happytransformer.happy_transformer -   Moving model to cpu\n",
      "05/20/2024 02:30:12 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iam del .\n"
     ]
    }
   ],
   "source": [
    "# commented\n",
    "# Example usage\n",
    "video_path = \"C:\\\\Users\\\\acer\\\\Desktop\\\\ok2.mp4\"\n",
    "output_folder = 'output_frames'\n",
    "# # interval =  3 # seconds\n",
    "print(predict_msg(video_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented\n",
    "# img='C:\\\\Users\\\\lenovo\\\\Downloads\\\\test2.png'\n",
    "# print(process_img(cv2.imread(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented\n",
    "# from flask import Flask, request, jsonify\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route(\"/api\", methods=[\"GET\"])\n",
    "# def predict():\n",
    "#     d={}\n",
    "#     inputchar=request.args['query']\n",
    "#     answer=predict_msg(str(inputchar))\n",
    "#     d['output']=answer\n",
    "#     return d\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented\n",
    "# from flask import Flask, request, jsonify\n",
    "# import requests\n",
    "# app = Flask(__name__)\n",
    "# @app.route(\"/api\", methods=[\"POST\"])\n",
    "# def predict():\n",
    "#     print(\"111111111111111111111111111111111\")\n",
    "#     inputchar=request.get_data()\n",
    "#     vid=base64.b64decode(inputchar)\n",
    "#     filename=\"C:\\\\Users\\\\acer\\\\Desktop\\\\salah.mp4\"\n",
    "#     with open(filename,'wb')as f:\n",
    "#         f.write(vid)\n",
    "#     print(\"2222222222222222222222222222222222\")\n",
    "#     response=predict_msg(filename)\n",
    "#     print(\"333333333333333333333333333333333\")\n",
    "#     #return response\n",
    "#     return \"Prediction result for {}\".format(filename)\n",
    " \n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "iam salah iam\n",
      "How are you?\n",
      "iam salah iam\n",
      "iam salah . \n",
      "How are you?\n",
      "iam ok .\n",
      "I love you.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/api\", methods=[\"GET\"])\n",
    "def predict():\n",
    "    d={}\n",
    "    inputchar=request.args['query']\n",
    "    answer=predict_msg(inputchar)\n",
    "    print(answer)\n",
    "    d['output']=answer\n",
    "    return d\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request, jsonify\n",
    "# import cv2\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route('/api', methods=['GET'])\n",
    "# def predict():\n",
    "#     video_path = request.args.get('query')\n",
    "#     if not video_path:\n",
    "#         return jsonify({'error': 'No video path provided'}), 400\n",
    "\n",
    "#     try:\n",
    "#         # فتح الفيديو باستخدام OpenCV\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "#         # التحقق من أنه تم فتح الفيديو بنجاح\n",
    "#         if not cap.isOpened():\n",
    "#             return jsonify({'error': 'Could not open video file'}), 400\n",
    "\n",
    "#         # الحصول على عدد الإطارات في الفيديو\n",
    "#         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#         # إغلاق الفيديو\n",
    "#         cap.release()\n",
    "\n",
    "#         # نتيجة المعالجة (مثال: عدد الإطارات في الفيديو)\n",
    "#         result = f'The video has {frame_count} frames.'\n",
    "        \n",
    "#         return jsonify({'output': result})\n",
    "\n",
    "#     except Exception as e:\n",
    "#         return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host='127.0.0.1', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
